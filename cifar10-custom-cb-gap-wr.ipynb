{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 데이터 전처리 수행.\n* 학습/검증/테스트 데이터 세트로 나누고 원-핫 인코딩 수행","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\nimport random as python_random\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.datasets import cifar10\n\n# seed 를 설정해서 학습시마다 동일한 결과 유도. 불행히도 의도한 대로 동작하지 않음. \ndef set_random_seed(seed_value):\n    np.random.seed(seed_value)\n    python_random.seed(seed_value)\n    tf.random.set_seed(seed_value)\n\n# 0 ~ 1사이값의 float32로 변경하는 함수\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\n# 0 ~ 1사이값 float32로 변경하는 함수 호출 한 뒤 OHE 적용 \ndef get_preprocessed_ohe(images, labels):\n    images, labels = get_preprocessed_data(images, labels)\n    # OHE 적용 \n    oh_labels = to_categorical(labels)\n    return images, oh_labels\n\n# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \ndef get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n    \n    # 학습 데이터를 검증 데이터 세트로 다시 분리\n    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n    \n    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels ) \n\n\n# random seed는 2021로 고정.\nset_random_seed(2021)\n# CIFAR10 데이터 재 로딩 및 Scaling/OHE 전처리 적용하여 학습/검증/데이터 세트 생성. \n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\nprint(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\n\nprint(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-03T17:11:27.619977Z","iopub.execute_input":"2022-01-03T17:11:27.620230Z","iopub.status.idle":"2022-01-03T17:11:37.247095Z","shell.execute_reply.started":"2022-01-03T17:11:27.620201Z","shell.execute_reply":"2022-01-03T17:11:37.244533Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:11:37.248588Z","iopub.execute_input":"2022-01-03T17:11:37.248972Z","iopub.status.idle":"2022-01-03T17:11:37.254223Z","shell.execute_reply.started":"2022-01-03T17:11:37.248924Z","shell.execute_reply":"2022-01-03T17:11:37.253303Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 모델 생성 함수 생성","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ndef create_model(verbose=False):\n    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n    #x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\n    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(filters=64, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=64, kernel_size=3, padding='same')(x)\n    x = Activation('relu')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n\n    x = Conv2D(filters=128, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=128, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n\n    # cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\n    x = Flatten(name='flatten')(x)\n    x = Dropout(rate=0.5)(x)\n    x = Dense(300, activation='relu', name='fc1')(x)\n    x = Dropout(rate=0.3)(x)\n    output = Dense(10, activation='softmax', name='output')(x)\n\n    model = Model(inputs=input_tensor, outputs=output)\n    \n    if verbose:\n        model.summary()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:18:40.030394Z","iopub.execute_input":"2022-01-03T15:18:40.030768Z","iopub.status.idle":"2022-01-03T15:18:40.046059Z","shell.execute_reply.started":"2022-01-03T15:18:40.030631Z","shell.execute_reply":"2022-01-03T15:18:40.045345Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"create_model(verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:18:45.377317Z","iopub.execute_input":"2022-01-03T15:18:45.377567Z","iopub.status.idle":"2022-01-03T15:18:47.825974Z","shell.execute_reply.started":"2022-01-03T15:18:45.377538Z","shell.execute_reply":"2022-01-03T15:18:47.825279Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Learning Rate와 Early Stopping을 위한 Callback 생성 \n* Learning rate 동적 변경은 ReduceLROnPlateau,  모델 Ealry Stopping은 EarlyStopping Callback을 이용.  ","metadata":{}},{"cell_type":"code","source":"# ModelCheckpoint를 동작시키기 전에 기존 저장된 모델은 모두 삭제 \n!rm *.hdf5","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:18:52.198787Z","iopub.execute_input":"2022-01-03T15:18:52.199042Z","iopub.status.idle":"2022-01-03T15:18:52.888904Z","shell.execute_reply.started":"2022-01-03T15:18:52.199014Z","shell.execute_reply":"2022-01-03T15:18:52.887973Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nmodel = create_model()\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# validation loss가 향상되는 모델만 저장.\nmcp_cb = ModelCheckpoint(filepath='/kaggle/working/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', \n                         save_best_only=True, save_weights_only=True, mode='min', period=1, verbose=0)\n\n# 5번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', verbose=1)\n# 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\nely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n\n\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=32, epochs=30, shuffle=True,\n                    validation_data=(val_images, val_oh_labels),  \n                    callbacks=[mcp_cb, rlr_cb, ely_cb] )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:18:56.505974Z","iopub.execute_input":"2022-01-03T15:18:56.506260Z","iopub.status.idle":"2022-01-03T15:22:53.108142Z","shell.execute_reply.started":"2022-01-03T15:18:56.506229Z","shell.execute_reply":"2022-01-03T15:22:53.107270Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:28:16.715103Z","iopub.execute_input":"2022-01-03T15:28:16.717110Z","iopub.status.idle":"2022-01-03T15:28:18.388238Z","shell.execute_reply.started":"2022-01-03T15:28:16.717055Z","shell.execute_reply":"2022-01-03T15:28:18.387493Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!ls -lia","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:28:20.631210Z","iopub.execute_input":"2022-01-03T15:28:20.631461Z","iopub.status.idle":"2022-01-03T15:28:21.347256Z","shell.execute_reply.started":"2022-01-03T15:28:20.631434Z","shell.execute_reply":"2022-01-03T15:28:21.346397Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 최적 weight를 모델로 재 로딩한 다음에 테스트 데이터로 다시 평가","metadata":{}},{"cell_type":"code","source":"# save_weights_only로 훈련을 돌렸기 때문에, 다시 모델을 만들어 줘야한다.\nmodel = create_model()\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n# 껍데기만 다 만들고 weights만 여기서 로드 해준다.\nmodel.load_weights('/kaggle/working/weights.19-0.51.hdf5')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:30:56.091218Z","iopub.execute_input":"2022-01-03T15:30:56.091546Z","iopub.status.idle":"2022-01-03T15:30:56.268378Z","shell.execute_reply.started":"2022-01-03T15:30:56.091505Z","shell.execute_reply":"2022-01-03T15:30:56.267647Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)\n# loss와 accuracy는 반드시 정비례 하지는 않는다. gradient descent는 loss만 줄이는데 혈안이 되어있다","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:30:59.040574Z","iopub.execute_input":"2022-01-03T15:30:59.040986Z","iopub.status.idle":"2022-01-03T15:31:00.647724Z","shell.execute_reply.started":"2022-01-03T15:30:59.040951Z","shell.execute_reply":"2022-01-03T15:31:00.646534Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history(history):\n    plt.figure(figsize=(8, 4))\n    plt.yticks(np.arange(0, 1, 0.05))\n    plt.xticks(np.arange(0, 30, 2))\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\n    \nshow_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T15:32:49.190723Z","iopub.execute_input":"2022-01-03T15:32:49.191287Z","iopub.status.idle":"2022-01-03T15:32:49.419987Z","shell.execute_reply.started":"2022-01-03T15:32:49.191244Z","shell.execute_reply":"2022-01-03T15:32:49.419169Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 필터의 개수를 증가시켜 테스트\n* Conv Layer의 필터를 기존보다 2배씩 증가\n* 맨 마지막 Conv는 512로 대폭 증가","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\n# 512 filters Conv layer 추가\nx = Conv2D(filters=512, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:04:55.049137Z","iopub.execute_input":"2022-01-03T16:04:55.049386Z","iopub.status.idle":"2022-01-03T16:04:57.627366Z","shell.execute_reply.started":"2022-01-03T16:04:55.049357Z","shell.execute_reply":"2022-01-03T16:04:57.626565Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 5번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', verbose=1)\n# 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\nely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n\n\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=32, epochs=30, shuffle=True,\n                    validation_data=(val_images, val_oh_labels),  \n                    callbacks=[rlr_cb, ely_cb] )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:09:07.851459Z","iopub.execute_input":"2022-01-03T16:09:07.851993Z","iopub.status.idle":"2022-01-03T16:14:08.943684Z","shell.execute_reply.started":"2022-01-03T16:09:07.851956Z","shell.execute_reply":"2022-01-03T16:14:08.942993Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history(history):\n    plt.figure(figsize=(8, 4))\n    plt.yticks(np.arange(0, 1, 0.05))\n    plt.xticks(np.arange(0, 30, 2))\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\n\nshow_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:18:32.111285Z","iopub.execute_input":"2022-01-03T16:18:32.111531Z","iopub.status.idle":"2022-01-03T16:18:34.048200Z","shell.execute_reply.started":"2022-01-03T16:18:32.111503Z","shell.execute_reply":"2022-01-03T16:18:34.047503Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 맨 마지막 Conv + MaxPooling 대신 맨 마지막 Conv의 Strides를 2로 하여 Feature map 조정","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\n# 512 filters Conv layer 추가하되 이후 MaxPooling을 적용하지 않고 strides는 2로 변경하여 출력 feature map 크기 조정\nx = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(x) # 이때의 padding = 'same'은 stride가 2일 때 feature map 일부 [끝]에 access하지 못하는걸 막아준다. \nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:11:48.425914Z","iopub.execute_input":"2022-01-03T17:11:48.426181Z","iopub.status.idle":"2022-01-03T17:11:50.912726Z","shell.execute_reply.started":"2022-01-03T17:11:48.426153Z","shell.execute_reply":"2022-01-03T17:11:50.912059Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 5번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', verbose=1)\n# 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\nely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n\n\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=32, epochs=30, shuffle=True,\n                    validation_data=(val_images, val_oh_labels),  \n                    callbacks=[rlr_cb, ely_cb] )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:11:55.875134Z","iopub.execute_input":"2022-01-03T17:11:55.875392Z","iopub.status.idle":"2022-01-03T17:16:23.961998Z","shell.execute_reply.started":"2022-01-03T17:11:55.875363Z","shell.execute_reply":"2022-01-03T17:16:23.961310Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history(history):\n    plt.figure(figsize=(8, 4))\n    plt.yticks(np.arange(0, 1, 0.05))\n    plt.xticks(np.arange(0, 30, 2))\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\nshow_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:16:37.576411Z","iopub.execute_input":"2022-01-03T17:16:37.576843Z","iopub.status.idle":"2022-01-03T17:17:21.910166Z","shell.execute_reply.started":"2022-01-03T17:16:37.576805Z","shell.execute_reply":"2022-01-03T17:17:21.909488Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Global Average Pooling의 적용\n* Global Average Pooling은 피처맵의 가로x세로의 특정 영역을 Sub sampling 않고, 채널별로 평균 값을 추출\n* 충분히 Feature map의 채널수가 많을 경우 이를 적용. 채널수가 적다면 Flatten이 유리. \n* Flatten-> Classification Dense Layer로 이어지면서 많은 파라미터들로 인한 오버피팅 유발 가능성 증대 및 학습 시간 늘어남. \n* 맨 마지막 Feature Map의 채널 수가 충분히 크다면 GlobalAveragePooling2D를 적용하여 Flatten을 제거하는데 더 유리 할 수 있음. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\n# Flatten 대신 Global AveragePooling 을 적용. \nx = GlobalAveragePooling2D()(x) # Flatten 했을 때의 오버피팅 가능성을 줄일 수 있따\nx = Dropout(rate=0.5)(x)\nx = Dense(50, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.2)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:17:28.638260Z","iopub.execute_input":"2022-01-03T17:17:28.638746Z","iopub.status.idle":"2022-01-03T17:17:28.977324Z","shell.execute_reply.started":"2022-01-03T17:17:28.638708Z","shell.execute_reply":"2022-01-03T17:17:28.976638Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 5번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', verbose=1)\n# 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\nely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n\n\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=32, epochs=30, shuffle=True,\n                    validation_data=(val_images, val_oh_labels),  \n                    callbacks=[rlr_cb, ely_cb] )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:17:31.848276Z","iopub.execute_input":"2022-01-03T17:17:31.848543Z","iopub.status.idle":"2022-01-03T17:22:26.819022Z","shell.execute_reply.started":"2022-01-03T17:17:31.848514Z","shell.execute_reply":"2022-01-03T17:22:26.818320Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)\nshow_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:34:15.694445Z","iopub.execute_input":"2022-01-03T17:34:15.694729Z","iopub.status.idle":"2022-01-03T17:34:18.760799Z","shell.execute_reply.started":"2022-01-03T17:34:15.694691Z","shell.execute_reply":"2022-01-03T17:34:18.759064Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### 가중치 규제(Weight Regularizations)\n* 개별 layer별로 tensorflow.keras.regularizers의 l1, l2, l1_l2 를 이용하여 가중치 규제를 적용할 수 있음. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(0.00001))(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(0.00001))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5))(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\n# 512 filters Conv layer 추가하되 이후 MaxPooling을 적용하지 않고 strides는 2로 변경하여 출력 feature map 크기 조정\nx = Conv2D(filters=512, kernel_size=3, strides=2, padding='same', kernel_regularizer=l2(1e-5))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', kernel_regularizer=l2(1e-5), name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:34:21.100639Z","iopub.execute_input":"2022-01-03T17:34:21.101233Z","iopub.status.idle":"2022-01-03T17:34:21.257029Z","shell.execute_reply.started":"2022-01-03T17:34:21.101194Z","shell.execute_reply":"2022-01-03T17:34:21.256310Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 5번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', verbose=1)\n# 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\nely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n\n\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=32, epochs=30, shuffle=True,\n                    validation_data=(val_images, val_oh_labels),  \n                    callbacks=[rlr_cb, ely_cb] )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:34:24.098069Z","iopub.execute_input":"2022-01-03T17:34:24.098329Z","iopub.status.idle":"2022-01-03T17:39:03.751712Z","shell.execute_reply.started":"2022-01-03T17:34:24.098301Z","shell.execute_reply":"2022-01-03T17:39:03.750703Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)\nshow_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:40:06.561808Z","iopub.execute_input":"2022-01-03T17:40:06.562060Z","iopub.status.idle":"2022-01-03T17:40:51.299284Z","shell.execute_reply.started":"2022-01-03T17:40:06.562033Z","shell.execute_reply":"2022-01-03T17:40:51.298629Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}